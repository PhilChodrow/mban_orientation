{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro to Pandas, Data Manipulation and Visualization in Python\n",
    "In this section, we will learn and practice how to read in data, conduct data manipulation and visualization in `Python`. In particular, we will be learning the `Pandas` package, which provides a fast and powerful interface to dataframes. \n",
    "\n",
    "## Pandas \n",
    "<img src=\"https://c402277.ssl.cf1.rackcdn.com/photos/13100/images/featured_story/BIC_128.png?1485963152\" align=\"right\">\n",
    "`Pandas` is a library that provides high-performance, easy-to-use data structures and data analysis tools for `Python`.\n",
    "\n",
    "\n",
    "\n",
    "Let's load the package `pandas` as well as `numpy`, and `matplotlib` for visualization later. The next few parameters set up the inline plotting to look nicely for the notebook. This is standard preamble for data processing in `ipython` notebooks that you can use in the future. There are some other variations such as giving `matplotlib` the `ggplot` theme from `R` if you wish (add `plt.style.use('ggplot')`). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "<img  width=\"200\" height=\"40\" src=\"https://upload.wikimedia.org/wikipedia/en/thumb/9/9f/Instacart_logo_and_wordmark.svg/1280px-Instacart_logo_and_wordmark.svg.png\" align=\"left\">\n",
    "<br>\n",
    "### Instacart orders\n",
    "\n",
    "\n",
    "In this problem, we'll use the dataset from Instacart.com (https://www.instacart.com/datasets/grocery-shopping-2017), a grocery delivery service that connects customers with Personal Shoppers who pick up and deliver the groceries from local stores. The open data contains order, product, and aisles detailed information. We took a 5% sample of orders in this tutorial.\n",
    "\n",
    "\n",
    "\n",
    "### Read in Data\n",
    "Now let's read in a csv file for the dataset `orders.csv` and `orders_products.csv` using the `read_csv` function in pd. Index is very important in Pandas for reasons we will talk about later (subset, merge, ...). Let's specify the index when we read in the data with `index_col = ` parameter. \n",
    "\n",
    "To get a glimpse of the data, you can do:\n",
    "* `.shape` to look at the dimension / size / shape of the dataframe,\n",
    "* `.describe()` to see a summary of the data,\n",
    "* `.head()` to view first 5 rows, or you can do it with `[:5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('data/orders.csv', index_col='order_id');\n",
    "print(orders.shape)\n",
    "print(orders.describe())\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_products = pd.read_csv('data/orders_products.csv', index_col='order_id');\n",
    "orders_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Indexing\n",
    "\n",
    "After reading in the datasets and taking a look at the description or the first few rows, we are interested in some basic dataframe manipulations. \n",
    "* **Subset columns:** To select a column, we can:\n",
    "    1. index with the name of the column as a string, \n",
    "    2. use the attribute operator . on the column name,\n",
    "    3. use the `loc[:, ]` function on the column name,\n",
    "    4. use the `iloc[:, ]` function on the column index (remember zero indexing in Python!)\n",
    "\n",
    "In this example we take the `order_hour_of_day` column using each of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orders['order_hour_of_day'].head())\n",
    "print(orders.order_hour_of_day.head())\n",
    "print(orders.loc[:, 'order_hour_of_day'].head())\n",
    "print(orders.iloc[:, 4].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select multiple columns by indexing the list of columns you would like to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders[['order_dow', 'order_hour_of_day']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Subset rows**: You can subset the rows of a dataframe by \n",
    "    1. `iloc[]`: based on the row numbers\n",
    "    2. `loc[]`: based on index value\n",
    "    3. `[]` with a logical condition\n",
    "\n",
    "Let's look at the following examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first 5 rows, use `iloc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.iloc[range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subset based on the index value, use the `loc` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.loc[[1076138,1609528]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want to look at the order hour of day being 6pm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_18pm = orders[orders['order_hour_of_day'] == 18]\n",
    "print(orders_18pm.shape)\n",
    "orders_18pm[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example use case could be that if we want to only keep observations with non-NA values for `days_since_prior_order`, we can use the `isnull()` function which returns a boolean array for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_noNA = orders[~orders['days_since_prior_order'].isnull()];\n",
    "orders_noNA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Explore `aisles` and `products` data\n",
    "Please read in the `aisles.csv` and `products.csv` files. Answer the following questions:\n",
    "* How many distinct aisles are there?\n",
    "* What's the name for aisle_id = 61? \n",
    "* How many products are there in aisle_id = 61?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Let's try to look at the relationship between the day-of-the-week and the hour-of-day for all orders. We can tabulate them by using the `crosstab()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_counts = pd.crosstab(orders['order_hour_of_day'], orders['order_dow'])\n",
    "orders_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting way to look at this data is to plot the distribution of hour-of-day by different day-of-the-week. A `Pandas` dataframe has some plot functions that can be called directly on it. For example, to do a line plot of the counts by each, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_counts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good already, without us needing to supply any arguments to the plot function. Monday and Sundays seem to be the days with more orders placed than other days. It also seems like there is a little bump on Monday, at around 9 to 10am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines for the other days seem to be overlapping and a bit hard to discern, so maybe it's a good idea to separate out the lines in multiple plots (think about the `facet` in `ggplot`). Try the following, which groups the counts by the two variables, count the sizes, and gives a plot separated by the first variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.groupby(['order_dow','order_hour_of_day']).size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful, but the labels seem a bit off. To slightly improve on this, we can do a loop of subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1,7, sharey=True)\n",
    "for i in range(7):\n",
    "    df = orders[orders['order_dow']==i].groupby('order_hour_of_day').size()\n",
    "    df.index.name = 'hour'\n",
    "    df.plot(ax=axarr[i], title='Day ' + str(i))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question that might be interesting is to look at the median, quantile, etc. of the hour of day. We can use a `boxplot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.boxplot(column='order_hour_of_day', by='order_dow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a heatmap could also be useful? To make a heatmap, we can use the `pcolor()` function. We provide the data and the color mapping as arguments to the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(orders_counts, cmap=plt.cm.Blues)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: visualize the relationship between `days_since_prior_order` and `order_day_of_week`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in looking at the `days_since_prior_order` variable and the `order_day_of_week` variable for the orders that are placed between 9am to 5pm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Group, Summarize, and Sort\n",
    "\n",
    "Suppose we are intereted in knowing something on the individual user level. For example, what's the total number of orders each user had? We can use the `groupby` and `size`. For a single variable this achieves similar effect as `value_counts()` function on that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.groupby('user_id').size()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['user_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort it further by the `sort_values()` function (and specifying `ascending = False` for decreasing order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.groupby('user_id').size().sort_values(ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, you can use the `agg` for aggregating specific summary statistic. \n",
    "* You can supply a single type and it will be performed on all variables: for example, getting the mean of each variable on each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.groupby('user_id').agg('mean')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* or supply a dictionary that the specfic variable as key: here only summarizses the mean of the `order_hour_of_day`, and the maximum of the `days_since_prior_order`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.groupby('user_id').agg({'order_hour_of_day': 'mean', 'days_since_prior_order': 'max'})[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Describing User Patterns\n",
    "\n",
    "From the `orders_products` dataframe, answer the following questions: \n",
    "* Are there more products that are reordered, or never ordered again?\n",
    "* Which `product_id` is the most frequently ordered? \n",
    "* What is that product called from the `products` dataframe?\n",
    "* Is there a relationship between the order when a product is added to cart (`add_to_cart_order`), and whether a product is reordered? \n",
    "* What about the relationship between whether the product is reordered and the total number of items in the order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Merge and Join\n",
    "\n",
    "With many separate dataframes, it is often useful to join them to understand the relationship between variables and also to create additional features in predictions. Pandas provides high-performance, in memory join operations that are similar to relational databases such as SQL.\n",
    "\n",
    "When the two dataframes are both indexed by the same variable that you want to join on, it is easy: use the `join` function on the left dataframe, and the right data frame is supplied as second argument.\n",
    "\n",
    "If the current index is not the right variable, we can first reindex the data using the `set_index` command:\n",
    "```python\n",
    "orders_products=orders_products.set_index('order_id')\n",
    "orders=orders.set_index('order_id')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_products_joined = orders_products.join(orders);\n",
    "orders_products_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the two columns do not have the same index, we can use the more flexible `merge` function:\n",
    "* the `left_on` and `right_on` options specify the column names to be joined on\n",
    "* if the variable to be joined on is an index, set `left_index` / `right_index` to `True`. \n",
    "* Finally, the `how` option allows you to specify the type of joins (left, right, inner, outer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_desc = orders_products.merge(products, left_on='product_id', right_index=True, how='left')\n",
    "order_products_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we can look at the most popular products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_products_desc['product_name'].value_counts()[:10].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Summarizing orders by aisle information\n",
    "\n",
    "We are interested in knowing which aisles are *LEAST* popular and can make management decisions based on that. Try to answer the following questions:\n",
    "* Which aisle has the least number of products ordered from?\n",
    "* What about only among the reordered products?\n",
    "\n",
    "\n",
    "**Challenge:** Suppose there was a software glitch that all products with the `add_to_cart_order` more than 30 was not correctly charged. What percent of all orders are affected by this glitch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from SQL databases\n",
    "\n",
    "(Note: The content for this section is adapted from the Pandas Cookbook Chapter 9.)\n",
    "\n",
    "Pandas can read from HTML, JSON, SQL, Excel, HDF5, Stata, and a few other things. We'll talk about reading data from SQL databases now.\n",
    "\n",
    "You can read data from a SQL database using the `pd.read_sql` function. `read_sql` will automatically convert SQL column names to DataFrame column names.\n",
    "\n",
    "`read_sql` takes 2 arguments: a `SELECT` statement, and a database connection object. It means you can read from *any* kind of SQL database -- it doesn't matter if it's MySQL, SQLite, PostgreSQL, or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"data/weather_2012.sqlite\")\n",
    "df = pd.read_sql(\"SELECT * from weather_2012 LIMIT 5\", con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! If you are familiar with SQL type statements, you can try some advanced `SELECT` statements; otherwise, just select everything you need and do the data cleaning in Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data Ready for SciKitLearn \n",
    "\n",
    "Having a cleaned `Pandas` dataframe does not allow you to run through machine learning packages directly yet. The dataframe may need to be appropriately transformed (onehot encoded for categorical variables, scaled, etc.). Both Pandas and scikit-learn offer some useful preprocessing functions.\n",
    "\n",
    "`pd.get_dummies` takes the dataframe, and a list of categorical columns to be converted into a dummified dataframe. See the following example, where we take the `orders_products_aisles` dataframe that we merged earlier, and convert the string categorical variable `aisle` to be onehot encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dum = pd.get_dummies(orders_products_aisles.drop('reordered', axis=1) , columns=['aisle'])\n",
    "X_dum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can run it through sklearn, which will be the topic of tomorrow's lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "Y = order_products_desc['reordered']\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_dum, Y)\n",
    "clf.predict_proba(X_dum)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other useful references\n",
    "\n",
    "* A useful cheatsheet: https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\n",
    "* The mapping between `R` commands and `Pandas` can be found here, if you are coming from a more `R`-type background:\n",
    "https://pandas.pydata.org/pandas-docs/version/0.18.1/comparison_with_r.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.rantchic.com/wp-content/uploads/2014/10/panda.jpg\" align=\"left\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
