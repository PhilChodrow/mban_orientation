{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Machine Learning in Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take a look at how to conduct the whole machine learning pipeline in python with scikit-learn and pandas.\n",
    "\n",
    "We'll use Homework 3 from 15.680 as the guideline for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: MAGIC Gamma Telescope\n",
    "https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope\n",
    "\n",
    "The data represents registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks through the atmosphere and gets recorded in the detector, allowing reconstruc- tion of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera. Depending on the energy of the primary gamma, a total of few hundreds to some 10000 Cherenkov photons get collected, in patterns (called the shower image), allowing to discriminate statistically those caused by primary gammas (signal) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (background). The aim is build a model that can distinguish between the signal and background cases.\n",
    "\n",
    "Attribute Information:\n",
    "1. fLength: continuous; major axis of ellipse [mm]\n",
    "2. fWidth: continuous; minor axis of ellipse [mm]\n",
    "3. fSize: continuous; 10-log of sum of content of all pixels [in ;phot]\n",
    "4. fConc: continuous; ratio of sum of two highest pixels over fSize [ratio]\n",
    "5. fConc1: continuous; ratio of highest pixel over fSize [ratio]\n",
    "6. fAsym: continuous; distance from highest pixel to center, projected onto major axis [mm] \n",
    "7. fM3Long: continuous; 3rd root of third moment along major axis [mm]\n",
    "8. fM3Trans: continuous; 3rd root of third moment along minor axis [mm]\n",
    "9. fAlpha: continuous; angle of major axis with vector to origin [deg]\n",
    "10. fDist: continuous; distance from origin to center of ellipse [mm] \n",
    "11. class: g,h; gamma (signal), hadron (background)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"magic04.csv\", header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_orig = df.iloc[:, -1]\n",
    "y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y is labelled g and h. Let's transform that to 0/1 labels. Luckily sklearn has an easy function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_orig)\n",
    "le.transform(y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = le.transform(y_orig)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split the data into training, validation and test. Again, sklearn has a function for this, it's basically the same as `splitobs` from MLDataUtils.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to split the training to get the actual training set and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_X, vl_X, tr_y, vl_y = train_test_split(train_X, train_y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start running our methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "m = LogisticRegression()\n",
    "m.fit(tr_X, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "print accuracy_score(vl_y, m.predict(vl_X))\n",
    "print roc_auc_score(vl_y, m.predict_proba(vl_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logistic regression is regularized, so let's try it out with different regularization types and amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "for penalty in ['l1', 'l2']:\n",
    "    for C in [1e-2, 1e0, 1e2, 1e4, 1e6, 1e8]:\n",
    "        m = LogisticRegression(penalty=penalty, C=C)\n",
    "        m.fit(tr_X, tr_y)\n",
    "        score = roc_auc_score(vl_y, m.predict_proba(vl_X)[:, 1])\n",
    "        print penalty, '\\t', str(C).rjust(12), '\\t', score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_penalty = penalty\n",
    "            best_C = C\n",
    "print best_penalty, best_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = LogisticRegression(penalty=penalty, C=C)\n",
    "m.fit(train_X, train_y)\n",
    "print accuracy_score(test_y, m.predict(test_X))\n",
    "print roc_auc_score(test_y, m.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try cross validation to select the parameters. There's a really nice interface for this in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1e-2, 1e0, 1e2, 1e4, 1e6, 1e8]\n",
    "}\n",
    "model = GridSearchCV(LogisticRegression(), params, cv=5, scoring='roc_auc')\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can get performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print accuracy_score(test_y, model.predict(test_X))\n",
    "print roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning via regularization paths\n",
    "\n",
    "You've probably experienced that it's annoying trying to tune the regularization penalties in our various regression models. We don't know where to start our values, how far apart to space them, etc.\n",
    "\n",
    "There's another way of tuning these parameters that [some methods support](http://scikit-learn.org/stable/modules/grid_search.html#model-specific-cross-validation). Where possible it's usually easier to use these approaches.\n",
    "\n",
    "The core idea is that it turns out to be possible to train the model for **all** values of the regularization penalty at once, for not much more cost than training the model once normally (or sometimes even less cost than training it once!). This is called finding the regularization path of the model, and from there we are able to identify the best parameter value without having to worry about specifying a range of possible values.\n",
    "\n",
    "Let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "m = LogisticRegressionCV(scoring=\"roc_auc\")\n",
    "m.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then inspect the best parameter, the validation scores, as well as calculate the test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The best value of C\n",
    "m.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The different values tried for C\n",
    "m.Cs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The scores for C in each fold\n",
    "m.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print accuracy_score(test_y, m.predict(test_X))\n",
    "print roc_auc_score(test_y, m.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization\n",
    "\n",
    "We've seen that feature normalization can help, especially for linear regression-based methods. This is easy to do in sklearn with by using a `Pipeline` to compose steps in the model building process.\n",
    "\n",
    "We'll use it to build a model that standardizes the data before fitting (zeroing the mean and setting unit variance). The main advantage of the pipeline approach is that we don't have to worry about applying the same transformation to our datasets, or de-transforming results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), LogisticRegressionCV(scoring=\"roc_auc\"))\n",
    "pipeline.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print accuracy_score(test_y, pipeline.predict(test_X))\n",
    "print roc_auc_score(test_y, pipeline.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "\n",
    "We can use the same generic approaches to fit our other classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "params = {'min_samples_leaf': [10, 50, 100], 'max_depth': range(1, 11)}\n",
    "model = GridSearchCV(DecisionTreeClassifier(), params)\n",
    "model.fit(train_X, train_y)\n",
    "print accuracy_score(test_y, model.predict(test_X))\n",
    "print roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(train_X, train_y)\n",
    "print accuracy_score(test_y, model.predict(test_X))\n",
    "print roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "params = {'learning_rate': [0.1, 0.2, 0.3], 'n_estimators': [50, 100, 500]}\n",
    "model = GridSearchCV(GradientBoostingClassifier(), params)\n",
    "model.fit(train_X, train_y)\n",
    "print accuracy_score(test_y, model.predict(test_X))\n",
    "print roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression: Parkinsons Telemonitoring\n",
    "https://archive.ics.uci.edu/ml/datasets/parkinsons+telemonitoring\n",
    "\n",
    "This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson’s disease recruited to a six-month trial of a telemonitoring device for remote symptom progres- sion monitoring. The recordings were automatically captured in the patient’s homes.\n",
    "\n",
    "The columns in the table are subject number, subject age, subject gender, time interval from baseline recruitment date, motor UPDRS, total UPDRS, and 16 biomedical voice measures. Each row corresponds to one of 5,875 voice recording from these individuals. The main aim of the data is to predict the motor UPDRS scores (`motor_UPDRS`) from the 16 voice measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Conduct the same comparison of methods on the parkinsons dataset. You can and should refer to the examples above and also to your code from Homework 3 if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
